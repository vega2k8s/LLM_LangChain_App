{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5434d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4b78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70202ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"human\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff33d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000202531318E0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002025333E690> root_client=<openai.OpenAI object at 0x0000020252E084D0> root_async_client=<openai.AsyncOpenAI object at 0x0000020252E26FC0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c1a5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 언어 모델을 프로덕션 환경에서 효율적으로 운영할 수 있습니다.\\n\\nLangChain은 LangServe의 핵심 라이브러리로서, 대규모 언어 모델을 활용하여 애플리케이션을 구축하기 위한 프레임워크를 제공합니다. LangChain은 언어 모델을 중심으로 한 애플리케이션 개발을 간소화하고, 모델의 활용도를 높여줍니다.\\n\\nLangServe의 주요 기능은 다음과 같습니다:\\n\\n1. **언어 모델 배포**: LangServe를 사용하면 대규모 언어 모델을 쉽게 배포할 수 있습니다. 개발자는 모델을 한 번 훈련시킨 후, LangServe를 통해 다양한 환경에서 모델을 운영할 수 있습니다.\\n\\n2. **API 제공**: LangServe는 자동으로 API를 생성해 줍니다. 이를 통해 클라이언트 애플리케이션에서 언어 모델을 쉽게 호출하고 사용할 수 있습니다.\\n\\n3. **트래킹 및 모니터링**: LangServe는 모델의 성능을 모니터링하고, 요청 및 응답을 로깅하는 기능을 제공합니다. 이를 통해 개발자는 모델의 성능을 분석하고, 필요한 경우 모델을 업데이트하거나 개선할 수 있습니다.\\n\\n4. **보안**: LangServe는 데이터 암호화 및 접근 제어를 지원하여, 모델과 데이터를 안전하게 보호합니다.\\n\\n5. **확장성**: LangServe는 수평 확장성을 지원합니다. 따라서 트래픽이 증가하더라도 LangServe를 통해 배포된 모델은 안정적으로 운영될 수 있습니다.\\n\\n6. **다중 모델 지원**: LangServe는 여러 개의 언어 모델을 동시에 배포하고 관리할 수 있는 기능을 제공합니다. 이를 통해 개발자는 다양한 모델을 활용하여 애플리케이션을 구축할 수 있습니다.\\n\\nLangServe를 사용하는 이점은 다음과 같습니다:\\n\\n* 대규모 언어 모델을 쉽게 배포하고 관리할 수 있습니다.\\n* 프로덕션 환경에서 언어 모델을 효율적으로 운영할 수 있습니다.\\n* 모델의 성능을 모니터링하고 분석할 수 있습니다.\\n* 보안 및 확장성을 제공합니다.\\n\\nLangServe는 언어 모델을 활용하여 애플리케이션을 구축하는 개발자에게 매우 유용한 도구입니다. LangServe를 사용하면 개발자는 언어 모델의 배포 및 관리에 집중할 수 있으며, 애플리케이션 개발에 더 많은 시간과 자원을 집중할 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 461, 'prompt_tokens': 30, 'total_tokens': 491, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.260537515, 'prompt_time': 0.003171891, 'completion_time': 1.121844027, 'total_time': 1.125015918}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-d9d011f7-52c1-4bdd-b96c-f6ca743f9fe3', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--1dea90dc-4df9-4cb3-9f10-4edcef178497-0' usage_metadata={'input_tokens': 30, 'output_tokens': 461, 'total_tokens': 491, 'input_token_details': {}, 'output_token_details': {}}\n",
      "응답: LangServe는 개발자가 대규모 언어 모델(LLM)을 쉽게 배포하고 관리할 수 있도록 지원하는 오픈 소스 라이브러리입니다. LangServe를 사용하면 개발자는 언어 모델을 프로덕션 환경에서 효율적으로 운영할 수 있습니다.\n",
      "\n",
      "LangChain은 LangServe의 핵심 라이브러리로서, 대규모 언어 모델을 활용하여 애플리케이션을 구축하기 위한 프레임워크를 제공합니다. LangChain은 언어 모델을 중심으로 한 애플리케이션 개발을 간소화하고, 모델의 활용도를 높여줍니다.\n",
      "\n",
      "LangServe의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **언어 모델 배포**: LangServe를 사용하면 대규모 언어 모델을 쉽게 배포할 수 있습니다. 개발자는 모델을 한 번 훈련시킨 후, LangServe를 통해 다양한 환경에서 모델을 운영할 수 있습니다.\n",
      "\n",
      "2. **API 제공**: LangServe는 자동으로 API를 생성해 줍니다. 이를 통해 클라이언트 애플리케이션에서 언어 모델을 쉽게 호출하고 사용할 수 있습니다.\n",
      "\n",
      "3. **트래킹 및 모니터링**: LangServe는 모델의 성능을 모니터링하고, 요청 및 응답을 로깅하는 기능을 제공합니다. 이를 통해 개발자는 모델의 성능을 분석하고, 필요한 경우 모델을 업데이트하거나 개선할 수 있습니다.\n",
      "\n",
      "4. **보안**: LangServe는 데이터 암호화 및 접근 제어를 지원하여, 모델과 데이터를 안전하게 보호합니다.\n",
      "\n",
      "5. **확장성**: LangServe는 수평 확장성을 지원합니다. 따라서 트래픽이 증가하더라도 LangServe를 통해 배포된 모델은 안정적으로 운영될 수 있습니다.\n",
      "\n",
      "6. **다중 모델 지원**: LangServe는 여러 개의 언어 모델을 동시에 배포하고 관리할 수 있는 기능을 제공합니다. 이를 통해 개발자는 다양한 모델을 활용하여 애플리케이션을 구축할 수 있습니다.\n",
      "\n",
      "LangServe를 사용하는 이점은 다음과 같습니다:\n",
      "\n",
      "* 대규모 언어 모델을 쉽게 배포하고 관리할 수 있습니다.\n",
      "* 프로덕션 환경에서 언어 모델을 효율적으로 운영할 수 있습니다.\n",
      "* 모델의 성능을 모니터링하고 분석할 수 있습니다.\n",
      "* 보안 및 확장성을 제공합니다.\n",
      "\n",
      "LangServe는 언어 모델을 활용하여 애플리케이션을 구축하는 개발자에게 매우 유용한 도구입니다. LangServe를 사용하면 개발자는 언어 모델의 배포 및 관리에 집중할 수 있으며, 애플리케이션 개발에 더 많은 시간과 자원을 집중할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafbd5a1",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c34c649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\\n    ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 반드시 한글로 설명해주세요.\")\n",
    "    \"\"\")                                     \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d392dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL) prompt + llm 연결\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58cf5be",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM + OutputParser을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbba4772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL) prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084bfec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 컴퓨터가 데이터를 통해 스스로 학습하고 발전하는 과정입니다. 이를 쉽게 설명해 드리겠습니다.\n",
      "\n",
      "**인공지능 모델의 학습 단계**\n",
      "\n",
      "1. **데이터 수집**: 인공지능 모델을 학습시키기 위해 필요한 데이터를 수집합니다. 이 데이터는 문제에 대한 답을 포함하고 있어야 합니다.\n",
      "2. **데이터 전처리**: 수집한 데이터를 깨끗하고 체계적으로 정리합니다. 이를 통해 데이터의 품질을 높이고 모델의 학습 효율성을 개선합니다.\n",
      "3. **모델 초기화**: 인공지능 모델을 초기화합니다. 이는 모델의 가중치와 편향을 설정하는 과정입니다.\n",
      "4. **학습**: 모델이 데이터를 학습합니다. 이 과정에서 모델은 데이터의 패턴과 관계를 발견하고 가중치와 편향을 업데이트합니다.\n",
      "5. **예측**: 모델이 학습한 데이터를 바탕으로 예측을 수행합니다.\n",
      "6. **평가**: 모델의 성능을 평가합니다. 이를 통해 모델의 정확도와 효율성을 측정합니다.\n",
      "\n",
      "**인공지능 모델의 학습 방법**\n",
      "\n",
      "인공지능 모델의 학습 방법에는 여러 가지가 있습니다. 대표적인 방법은 다음과 같습니다.\n",
      "\n",
      "1. **지도 학습**: 레이블이 있는 데이터를 사용하여 모델을 학습시키는 방법입니다. 모델은 데이터의 패턴과 관계를 학습하여 새로운 데이터에 대한 예측을 수행합니다.\n",
      "2. **비지도 학습**: 레이블이 없는 데이터를 사용하여 모델을 학습시키는 방법입니다. 모델은 데이터의 패턴과 관계를 학습하여 데이터의 구조를 발견합니다.\n",
      "3. **강화 학습**: 모델이 환경과 상호작용하며 학습하는 방법입니다. 모델은 보상을 최대화하기 위해 행동을 선택합니다.\n",
      "\n",
      "**인공지능 모델의 학습 원리**\n",
      "\n",
      "인공지능 모델의 학습 원리는 다음과 같습니다.\n",
      "\n",
      "1. **오차 최소화**: 모델은 예측과 실제값 사이의 오차를 최소화하기 위해 학습합니다.\n",
      "2. **가중치 업데이트**: 모델은 가중치를 업데이트하여 오차를 최소화합니다.\n",
      "3. **반복 학습**: 모델은 데이터를 반복적으로 학습하여 성능을 개선합니다.\n",
      "\n",
      "예를 들어, 자율 주행 자동차의 경우, 카메라와 센서를 통해 수집한 데이터를 바탕으로 모델을 학습시킵니다. 모델은 도로와 차량의 패턴을 학습하여 자동차가 안전하게 주행할 수 있도록 도와줍니다.\n",
      "\n",
      "결론적으로, 인공지능 모델의 학습 원리는 데이터를 통해 모델이 스스로 학습하고 발전하는 과정입니다. 모델은 오차를 최소화하고 가중치를 업데이트하여 성능을 개선합니다. 이를 통해 인공지능 모델은 다양한 분야에서 활용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33153ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChain은 다양한 제품과 도구를 제공하여 개발자들이 인공지능(AI) 모델을 보다 쉽게 구축하고 활용할 수 있도록 지원합니다. 주요 제품에는 LangSmith, LangServe, LangChain CLI 등이 있습니다.\n",
      "\n",
      "1. **LangSmith**: \n",
      "   - 랭스미스는 랭체인에서 제공하는 통합 개발 환경(IDE)입니다. \n",
      "   - 개발자들이 랭체인 모델을 더 쉽게 개발, 테스트, 배포할 수 있도록 도와줍니다.\n",
      "   - LangSmith를 통해 개발자들은 워크플로우를 시각적으로 관리하고, 에이전트와 체인을 더 체계적으로 추적할 수 있습니다.\n",
      "\n",
      "2. **LangServe**:\n",
      "   - 랭서브는 랭체인에서 제공하는 API 서버입니다.\n",
      "   - LangServe를 사용하면 랭체인 애플리케이션을 쉽게 배포하고 관리할 수 있습니다.\n",
      "   - 개발자들은 LangServe를 통해 랭체인 모델을 RESTful API로 감싸서 다른 서비스나 애플리케이션에서 쉽게 사용할 수 있도록 할 수 있습니다.\n",
      "\n",
      "3. **LangChain CLI**:\n",
      "   - 랭체인 CLI는 명령어 기반 인터페이스를 제공하여 랭체인 모델과 애플리케이션을 관리할 수 있습니다.\n",
      "\n",
      "이러한 제품들은 개발자들이 랭체인 생태계 내에서 더욱 효율적으로 작업할 수 있도록 지원합니다. 각 제품은 랭체인 프레임워크의 일부로서, 개발자들이 AI 모델을 보다 효과적으로 구축, 테스트, 배포하는 데 중요한 역할을 합니다.\n",
      "\n",
      "예를 들어, 개발자가 자연어 처리(NLP) 기반의 챗봇을 개발한다고 가정해 봅시다. 이 경우, LangSmith를 사용하여 개발 환경을 설정하고, LangServe를 통해 챗봇 API를 배포할 수 있습니다. 이렇게 하면 챗봇의 개발, 테스트, 배포 과정이 보다 효율적이고 체계적으로 진행될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \": LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangSmith, LangServe 같은 Product가 있어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be77c69",
   "metadata": {},
   "source": [
    "### Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33721597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    \n",
    "    # 스트리밍 출력\n",
    "    #print(answer)\n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fbba4",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "* 두개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd14ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 한국 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 10문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dffbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 입력 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"액션\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa9bc7",
   "metadata": {},
   "source": [
    "### PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab54a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
