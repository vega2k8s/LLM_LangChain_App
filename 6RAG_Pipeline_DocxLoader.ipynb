{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5c1522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432dd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9491ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # 특정 경고 유형만 무시\n",
    "\n",
    "#  1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "#  3. DOCX 파일 로드 및 텍스트 추출 (Docx2txtLoader 활용)\n",
    "def load_docx(file_path):\n",
    "    \"\"\"DOCX 파일에서 텍스트를 추출하는 함수.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "#  4. 문서 분할 함수\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"텍스트를 지정된 크기의 청크로 분할하는 함수.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "#  5. 벡터 데이터베이스(FAISS) 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"텍스트 청크를 임베딩하고 FAISS 벡터 저장소에 저장.\"\"\"\n",
    "    try:\n",
    "        documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "#  6. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store):\n",
    "    \"\"\"LLM을 사용하여 검색된 문서 기반으로 답변 생성.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        # 프롬프트 로드 (RAG 최적화된 LangChain Hub 프롬프트 사용)\n",
    "        #prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        prompt = hub.pull(\"cheorhyeon/rag-prompt-korean\")\n",
    "        print(prompt)\n",
    "\n",
    "        # RetrievalQA 체인 생성\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm, \n",
    "            retriever=vector_store.as_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "\n",
    "        # LLM 응답 생성\n",
    "        ai_message = qa_chain.invoke({\"query\": query})\n",
    "        print(ai_message)\n",
    "        return ai_message[\"result\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b83808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 로드 완료\n",
      "문서 분할 완료: 296개 청크 생성\n",
      "벡터 저장소 생성 완료\n",
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'cheorhyeon', 'lc_hub_repo': 'rag-prompt-korean', 'lc_hub_commit_hash': '23289ae9d525acc8cd52f2e7be1b4a2b09f527f7fa5d42287cac0ab824a4b3ca'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"\\n당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다.\\n주어진 문맥(Context)에서 질문(Question)에 답하세요.\\n답을 모르면 '주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다'라고 답하세요.\\n모든 답변은 한글로 작성하되, 기술 용어나 이름은 번역하지 마세요.\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n#Question: {question}\\n\\n#Context: {context}\\n'), additional_kwargs={})]\n",
      "{'query': '총수입금액 불산입에 대하여 설명해 주세요.', 'result': '총수입금액은 거주자가 해당 과세기간에 실제로 수입하거나 수입할 금액의 합계액을 말합니다. 총수입금액은 총급여액과 총연금액을 포함하며, 금전 외의 것을 수입할 때에는 그 수입금액을 그 거래 당시의 가액에 의하여 계산합니다. 총수입금액은 해당 과세기간의 소득금액을 계산할 때 중요한 요소이며, 관련 범위와 계산 방법은 대통령령에 의해 정의됩니다.'}\n",
      "\n",
      " AI의 답변:\n",
      "총수입금액은 거주자가 해당 과세기간에 실제로 수입하거나 수입할 금액의 합계액을 말합니다. 총수입금액은 총급여액과 총연금액을 포함하며, 금전 외의 것을 수입할 때에는 그 수입금액을 그 거래 당시의 가액에 의하여 계산합니다. 총수입금액은 해당 과세기간의 소득금액을 계산할 때 중요한 요소이며, 관련 범위와 계산 방법은 대통령령에 의해 정의됩니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"data/tax_with_table_short.docx\"\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    text = load_docx(docx_path)\n",
    "    print(\"문서 로드 완료\")\n",
    "    \n",
    "    # 2. 문서 분할\n",
    "    text_chunks = split_text(text)\n",
    "    print(f\"문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    vector_store = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store)\n",
    "    \n",
    "    # 6. AI 응답 출력\n",
    "    print(\"\\n AI의 답변:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227a822",
   "metadata": {},
   "source": [
    "### Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee000dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. 환경 변수 로드 및 API 키 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OpenAI API 키가 설정되지 않았습니다. .env 파일에 OPENAI_API_KEY를 추가하세요.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"\n",
    "    한국어 법률 문서의 구조를 고려한 텍스트 전처리 함수\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 전처리된 텍스트\n",
    "        \n",
    "    주요 처리 내용:\n",
    "        - 불필요한 공백 및 개행 정리\n",
    "        - 법조문 번호 정규화 (제1조, 제2조 등)\n",
    "        - 항 번호를 아라비아 숫자로 변환 (①→제1항)\n",
    "        - 호 번호 정규화\n",
    "    \"\"\"\n",
    "    # 연속된 공백을 하나의 공백으로 통일\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화: \"제 1 조\" -> \"제1조\" 형태로 통일\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 원문자 항 번호를 아라비아 숫자로 변환하여 검색 정확도 향상\n",
    "    # ①②③④⑤⑥⑦⑧⑨⑩ -> 제1항, 제2항, ... 제10항\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화: \"1. \" -> \"제1호 \" 형태로 변환\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 법률 문서에 최적화된 텍스트 분할 함수\n",
    "def advanced_split_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    법률 문서의 구조적 특성을 고려한 지능적 텍스트 분할\n",
    "    \n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        chunk_size (int): 각 청크의 목표 크기 (문자 수)\n",
    "        chunk_overlap (int): 청크 간 중복되는 문자 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 분할된 텍스트 청크들의 리스트\n",
    "        \n",
    "    특징:\n",
    "        - 법률 문서의 계층 구조(조>항>호>목)를 고려한 분할 우선순위\n",
    "        - 의미적 완성도를 유지하면서 분할\n",
    "        - 토큰 한도를 고려한 적절한 크기 설정\n",
    "    \"\"\"\n",
    "    # 텍스트 전처리 수행\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할 구분자들을 우선순위대로 설정\n",
    "    # 상위 구조부터 하위 구조 순서로 분할을 시도\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 단위 분할 (가장 우선)\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 단위 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 단위 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 단위 분할\n",
    "            \"\\n\\n\",  # 문단 단위 분할\n",
    "            \"\\n\",    # 줄 단위 분할\n",
    "            \". \",    # 문장 단위 분할\n",
    "            \" \",     # 단어 단위 분할\n",
    "            \"\"       # 문자 단위 분할 (최후 수단)\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. DOCX 파일 로딩 및 전처리 함수\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"\n",
    "    DOCX 파일을 로드하고 기본적인 텍스트 정리를 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): DOCX 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 정리된 텍스트\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: 파일 로딩 실패 시\n",
    "        ValueError: 텍스트 추출 실패 시\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Docx2txtLoader를 사용하여 DOCX 파일에서 텍스트 추출\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # 텍스트가 비어있는지 확인\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다. 파일이 비어있거나 손상되었을 수 있습니다.\")\n",
    "        \n",
    "        # 기본적인 텍스트 정리 작업\n",
    "        # 연속된 빈 줄을 두 개의 줄바꿈으로 통일\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # 연속된 공백과 탭을 하나의 공백으로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 배치 처리를 통한 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model, batch_size=30):\n",
    "    \"\"\"\n",
    "    텍스트 청크들을 배치 단위로 처리하여 벡터 저장소 생성\n",
    "    토큰 한도 초과 문제를 해결하기 위해 배치 처리 방식 적용\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): 분할된 텍스트 청크들\n",
    "        embedding_model: OpenAI 임베딩 모델 객체\n",
    "        batch_size (int): 한 번에 처리할 청크 수 (기본값: 30)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (FAISS 벡터 저장소, Document 객체들의 리스트)\n",
    "        \n",
    "    처리 과정:\n",
    "        1. 각 청크에 메타데이터 추가 (ID, 길이, 조항 정보 등)\n",
    "        2. 청크 크기가 너무 큰 경우 자동으로 제한\n",
    "        3. 배치 단위로 임베딩 생성하여 토큰 한도 문제 방지\n",
    "        4. FAISS merge 기능을 활용하여 배치별 결과 통합\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   총 {len(text_chunks)}개 청크를 {batch_size}개씩 배치 처리...\")\n",
    "        \n",
    "        # Document 객체들을 저장할 리스트 초기화\n",
    "        documents = []\n",
    "        \n",
    "        # 각 텍스트 청크를 Document 객체로 변환하면서 메타데이터 추가\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 청크 크기가 너무 큰 경우 제한 (토큰 한도 방지)\n",
    "            if len(chunk) > 2000:\n",
    "                chunk = chunk[:2000] + \"...\"\n",
    "                print(f\"   경고: 청크 {i}가 너무 커서 2000자로 제한했습니다.\")\n",
    "            \n",
    "            # 각 청크에 추가할 메타데이터 구성\n",
    "            metadata = {\n",
    "                'chunk_id': i,  # 청크 고유 번호\n",
    "                'chunk_length': len(chunk),  # 청크 길이\n",
    "                'chunk_type': 'legal_document'  # 문서 유형\n",
    "            }\n",
    "            \n",
    "            # 청크 내용에서 조항 정보 자동 추출\n",
    "            # \"제n조\" 패턴을 찾아 메타데이터에 추가\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            # Document 객체 생성하여 리스트에 추가\n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        # 배치별 벡터 저장소 생성 및 병합 과정\n",
    "        vector_store = None\n",
    "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # 문서들을 배치 크기만큼 나누어 처리\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            \n",
    "            print(f\"   배치 {batch_num}/{total_batches} 처리 중... ({len(batch_docs)}개 문서)\")\n",
    "            \n",
    "            # 첫 번째 배치인 경우 새로운 벡터 저장소 생성\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "            else:\n",
    "                # 이후 배치들은 기존 벡터 저장소에 병합\n",
    "                batch_vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "                vector_store.merge_from(batch_vector_store)\n",
    "        \n",
    "        print(\"   모든 배치 처리 완료\")\n",
    "        return vector_store, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"\n",
    "    단순한 키워드 매칭을 통한 문서 검색\n",
    "    벡터 검색과 상호 보완적으로 사용하여 검색 정확도 향상\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 반환할 상위 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 관련도 순으로 정렬된 Document 객체들\n",
    "        \n",
    "    검색 로직:\n",
    "        1. 질의와 문서의 단어 교집합 계산\n",
    "        2. 교집합 크기를 질의 단어 수로 나누어 정규화\n",
    "        3. 정확한 구문 매칭 시 보너스 점수 부여\n",
    "        4. 점수 기준으로 상위 k개 문서 반환\n",
    "    \"\"\"\n",
    "    # 질의를 소문자로 변환하고 단어 단위로 분할\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수를 계산할 리스트\n",
    "    scores = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # 문서 내용을 소문자로 변환하고 단어 단위로 분할\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 질의 단어와 문서 단어의 교집합 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        \n",
    "        # 기본 점수: 교집합 크기를 질의 단어 수로 나누어 정규화 (0~1 범위)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 보너스 점수: 질의 전체가 문서에 정확히 포함된 경우\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        # (점수, 인덱스, 문서) 튜플로 저장\n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수 기준으로 내림차순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수 (벡터 + 키워드 검색 결합)\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"\n",
    "    벡터 유사도 검색과 키워드 검색을 결합한 하이브리드 검색\n",
    "    두 검색 방법의 장점을 결합하여 더 정확한 검색 결과 제공\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 최종 반환할 문서 수\n",
    "        alpha (float): 벡터 검색 가중치 (0~1, 높을수록 벡터 검색 중시)\n",
    "        \n",
    "    Returns:\n",
    "        list: 종합 점수로 정렬된 상위 k개 Document 객체들\n",
    "        \n",
    "    검색 과정:\n",
    "        1. 벡터 유사도 검색으로 의미적으로 관련된 문서 찾기\n",
    "        2. 키워드 검색으로 정확한 용어 매칭 문서 찾기\n",
    "        3. 두 결과를 alpha 가중치로 결합\n",
    "        4. 중복 문서 처리 및 최종 점수 계산\n",
    "        5. 점수 순으로 정렬하여 상위 k개 반환\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색 수행\n",
    "    # 더 많은 후보를 가져와서 다양성 확보 (k*2개)\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)\n",
    "    \n",
    "    # 2. 키워드 기반 검색 수행\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 두 검색 결과를 점수와 함께 통합\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과에 점수 부여 (alpha 가중치 적용)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        # 문서 내용을 고유 키로 사용\n",
    "        doc_id = doc.page_content\n",
    "        # 순위가 높을수록 높은 점수 (1.0에서 시작하여 순위에 따라 감소)\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        \n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1,\n",
    "            'keyword_rank': None\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과에 점수 부여 ((1-alpha) 가중치 적용)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 벡터 검색에서 찾은 문서인 경우 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 키워드 검색에서만 찾은 새로운 문서인 경우 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'vector_rank': None,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 종합 점수 기준으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"\n",
    "    한국어 법률 문서 특성에 맞춘 전용 프롬프트 템플릿 생성\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: 법률 문서 질의응답을 위한 프롬프트 템플릿\n",
    "        \n",
    "    프롬프트 특징:\n",
    "        - 법조문 인용의 정확성 강조\n",
    "        - 전문 용어에 대한 쉬운 설명 요구\n",
    "        - 조항 간 연관성 설명 포함\n",
    "        - 실무적 적용 방법 제시\n",
    "        - 불확실한 내용에 대한 명시적 언급\n",
    "    \"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"\n",
    "    하이브리드 검색과 고성능 LLM을 결합한 질문 응답 시스템\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        \n",
    "    Returns:\n",
    "        dict: 답변, 참고 문서, 사용된 컨텍스트를 포함한 응답 딕셔너리\n",
    "        \n",
    "    처리 과정:\n",
    "        1. GPT-4o-mini 모델로 LLM 초기화 (높은 정확도)\n",
    "        2. 하이브리드 검색으로 관련 문서 7개 검색\n",
    "        3. 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        4. 법률 문서 전용 프롬프트 적용\n",
    "        5. LLM으로 최종 답변 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 고성능 LLM 모델 설정\n",
    "        llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",  # 정확도가 높은 모델 사용\n",
    "            temperature=0.1,  # 낮은 온도로 일관성 있는 답변 생성\n",
    "            max_tokens=1000   # 충분한 답변 길이 허용\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 검색\n",
    "        # k=7로 설정하여 충분한 컨텍스트 확보\n",
    "        # alpha=0.7로 설정하여 벡터 검색을 더 중시 (의미적 유사도 우선)\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        # 각 문서에 번호를 매겨 구분하기 쉽게 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서에 특화된 프롬프트 사용\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 최종 프롬프트 생성 (컨텍스트와 질문 삽입)\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM에 프롬프트 전달하여 답변 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 결과를 딕셔너리 형태로 반환\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 검색 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    검색된 문서들의 품질을 정량적으로 평가\n",
    "    \n",
    "    Args:\n",
    "        query (str): 원본 질의\n",
    "        retrieved_docs (list): 검색된 Document 객체들\n",
    "        \n",
    "    Returns:\n",
    "        float: 0~1 범위의 품질 점수 (1에 가까울수록 높은 품질)\n",
    "        \n",
    "    평가 기준:\n",
    "        1. 키워드 매칭률 (70% 가중치): 질의 단어가 문서에 포함된 비율\n",
    "        2. 문서 길이 적절성 (30% 가중치): 너무 짧거나 길지 않은 적절한 길이\n",
    "    \"\"\"\n",
    "    # 질의를 단어 단위로 분할하고 소문자 변환\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서별 품질 점수 계산\n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # 문서 내용을 단어 단위로 분할하고 소문자 변환\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수 계산\n",
    "        # 질의 단어 중 문서에 포함된 단어의 비율\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 계산\n",
    "        # 1000자를 기준으로 정규화 (1000자 이상이면 1.0점)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수 계산 (키워드 매칭 70% + 길이 적절성 30%)\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    # 전체 문서의 평균 품질 점수 반환\n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03db5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개선된 RAG 파이프라인 실행\n",
      "==================================================\n",
      "1. 문서 로드 중...\n",
      "   문서 로드 완료: 120,207 문자\n",
      "\n",
      "2. 문서 분할 중...\n",
      "   문서 분할 완료: 273개 청크 생성\n",
      "   평균 청크 길이: 477자, 최대 길이: 600자\n",
      "\n",
      "3. 임베딩 모델 초기화...\n",
      "   임베딩 모델 초기화 완료\n",
      "\n",
      "4. 벡터 저장소 생성 중...\n",
      "   총 273개 청크를 30개씩 배치 처리...\n",
      "   배치 1/10 처리 중... (30개 문서)\n",
      "   배치 2/10 처리 중... (30개 문서)\n",
      "   배치 3/10 처리 중... (30개 문서)\n",
      "   배치 4/10 처리 중... (30개 문서)\n",
      "   배치 5/10 처리 중... (30개 문서)\n",
      "   배치 6/10 처리 중... (30개 문서)\n",
      "   배치 7/10 처리 중... (30개 문서)\n",
      "   배치 8/10 처리 중... (30개 문서)\n",
      "   배치 9/10 처리 중... (30개 문서)\n",
      "   배치 10/10 처리 중... (3개 문서)\n",
      "   모든 배치 처리 완료\n",
      "   벡터 저장소 생성 완료\n",
      "\n",
      "5. 질의 실행 중...\n",
      "질의: 총수입금액 불산입에 대하여 설명해 주세요.\n",
      "하이브리드 검색 수행 중...\n",
      "검색된 관련 문서: 7개\n",
      "LLM 응답 생성 중...\n",
      "\n",
      "============================================================\n",
      "AI의 답변:\n",
      "============================================================\n",
      "총수입금액 불산입에 대한 설명은 다음과 같습니다. 총수입금액 불산입은 특정한 수입이나 금액이 사업소득금액을 계산할 때 총수입금액에 포함되지 않는 경우를 의미합니다. 이는 주로 세법에서 특정한 조건을 충족하는 경우에 적용됩니다.\n",
      "\n",
      "### 1. 관련 조항 인용 및 설명\n",
      "\n",
      "#### 1.1. 총수입금액의 정의\n",
      "- **제24조(총수입금액의 계산)** 제1항에 따르면, 거주자의 각 소득에 대한 총수입금액은 해당 과세기간에 수입하였거나 수입할 금액의 합계액으로 정의됩니다. 즉, 총수입금액은 특정 기간 동안 발생한 모든 수입의 총합을 의미합니다.\n",
      "\n",
      "#### 1.2. 불산입되는 금액\n",
      "- **제8항**에서는 원재료, 연료 등과 관련하여 매입, 수입 또는 사용함에 따라 부담하는 세액은 총수입금액에 산입하지 않는다고 명시하고 있습니다. 이는 사업 운영에 필수적인 비용이지만, 세액이 총수입금액에 포함되지 않도록 하여 과세의 형평성을 유지하려는 목적이 있습니다.\n",
      "  \n",
      "- **제9항**에서는 부가가치세의 매출세액도 총수입금액에 산입하지 않는다고 규정하고 있습니다. 이는 부가가치세가 소비자에게 전가되는 세금이기 때문에 사업자가 부담하는 세액이 총수입금액에 포함되지 않도록 하는 것입니다.\n",
      "\n",
      "- **제10항**에서는 석유판매업자가 환급받은 세액도 총수입금액에 산입하지 않는다고 명시하고 있습니다. 이는 특정 업종에 대한 세액 환급이 사업소득금액에 영향을 미치지 않도록 하기 위한 조치입니다.\n",
      "\n",
      "### 2. 실무적 적용 방법\n",
      "총수입금액 불산입 규정은 사업자가 세무 신고를 할 때 매우 중요합니다. 사업자는 다음과 같은 절차를 따라야 합니다:\n",
      "\n",
      "1. **세액 계산**: 매출세액, 환급세액 등을 정확히 계산하여 총수입금액에서 제외해야 합니다.\n",
      "2. **회계 처리**: 회계 장부에 이러한 세액을 별도로 기록하여, 총수입금액 계산 시 불산입 항목으로 처리해야 합니다.\n",
      "3. **세무 신고**: 세무 신고 시, 불산입 항목을 명확히 기재하여 세무 당국에 제출해야 합니다.\n",
      "\n",
      "### 3. 관련 조항 간의 연관성\n",
      "총수입금액 불산입 규정은 사업소득금액을 계산하는 데 있어 중요한 역할을 합니다. 제24조에서 정의된 총수입금액의 범위와 제8항, 제9항, 제10항에서 규정된 불산입 항목들은 서로 연결되어 있습니다. 즉, 총수입금액을 정확히 계산하기 위해서는 불산입 항목을 명확히 이해하고 적용해야 합니다.\n",
      "\n",
      "### 4. 불확실한 내용\n",
      "법률 문서에서 언급된 특정 세액이나 환급금의 구체적인 적용 기준이나 예외 사항은 대통령령에 의해 정해질 수 있습니다. 따라서, 구체적인 사례에 따라 다를 수 있으므로, 세무 전문가와 상담하거나 관련 법령을 참고하는 것이 필요합니다.\n",
      "\n",
      "============================================================\n",
      "검색 결과 요약:\n",
      "============================================================\n",
      "참고한 문서 조각 수: 7개\n",
      "컨텍스트 품질 점수: 0.19/1.00\n",
      "총 컨텍스트 길이: 3,108 문자\n",
      "\n",
      "============================================================\n",
      "참고한 문서 미리보기:\n",
      "============================================================\n",
      "\n",
      "[문서 1] . 다만, 원재료, 연료, 그 밖의 물품을 매입ㆍ수입 또는 사용함에 따라 부담하는 세액은 그러하지 아니하다. 제8항 「국세기본법」 제52조에 따른 국세환급가산금, 「지방세기본법」 제62조에 따른 지방세환급가산금, 그 밖의 과오납금(過誤納金)의 환급금에 대한 이자는 해당 과세기간의 소득금액을 계산할 때 총수입금액에 산입하지 아니한다.<개정 제2010호 제12...\n",
      "\n",
      "[문서 2] . 제3항 제1항에 따른 필요경비 불산입에 관하여 필요한 사항은 대통령령으로 정한다. [전문개정 제2009호 제12호 31.] [시행일: 제2025호 제1호 1.] 제33조제1항제1호 제33조의2(업무용승용차 관련 비용 등의 필요경비 불산입 특례) 제1항 제160조제3항에 따른 복식부기의무자가 해당 과세기간에 업무에 사용한 「개별소비세법」 제1조제2항제3호...\n",
      "\n",
      "[문서 3] 따라 계산한 금액 제2호 수입금액별 한도: 해당 사업에 대한 해당 과세기간의 수입금액(대통령령으로 정하는 수입금액만 해당한다) 합계액에 다음 표에 규정된 적용률을 적용하여 산출한 금액\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 처리할 DOCX 파일 경로 설정\n",
    "    docx_path = \"data/tax_with_table_short.docx\"\n",
    "    \n",
    "    print(\"개선된 RAG 파이프라인 실행\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1단계: 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\")\n",
    "    \n",
    "    # 2단계: 문서 분할\n",
    "    print(\"\\n2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=600, chunk_overlap=100)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 청크 크기 통계 분석 및 출력\n",
    "    chunk_lengths = [len(chunk) for chunk in text_chunks]\n",
    "    avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    max_length = max(chunk_lengths)\n",
    "    print(f\"   평균 청크 길이: {avg_length:.0f}자, 최대 길이: {max_length}자\")\n",
    "    \n",
    "    # 3단계: 임베딩 모델 초기화\n",
    "    print(\"\\n3. 임베딩 모델 초기화...\")\n",
    "    # 성능이 우수한 text-embedding-3-large 모델 사용\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",\n",
    "    )\n",
    "    print(\"   임베딩 모델 초기화 완료\")\n",
    "    \n",
    "    # 4단계: 벡터 저장소 생성\n",
    "    print(\"\\n4. 벡터 저장소 생성 중...\")\n",
    "    # 배치 크기 30으로 설정하여 토큰 한도 문제 방지\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model, batch_size=30)\n",
    "    print(\"   벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5단계: 질의 실행\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    #query = \"비과세소득의 종류에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
