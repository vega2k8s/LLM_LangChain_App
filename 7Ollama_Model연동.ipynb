{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " 'Alright, so I need to figure out what LangChain is. The user already '\n",
      " 'provided a pretty comprehensive breakdown, but maybe I can think through it '\n",
      " 'step by step.\\n'\n",
      " '\\n'\n",
      " \"First, I know that LangChain stands for Language Generation Chain. It's part \"\n",
      " 'of the LangChain framework, which is used for generating text. But how '\n",
      " 'exactly does it work? I remember something about transformers being involved '\n",
      " 'in this process.\\n'\n",
      " '\\n'\n",
      " 'So, transformers are a big thing in NLP, right? They help with tasks like '\n",
      " 'language modeling, attention mechanisms, and classification. In the context '\n",
      " \"of LangChain, maybe they're used for generating coherent responses or \"\n",
      " 'instructions.\\n'\n",
      " '\\n'\n",
      " \"But wait, isn't there more to it than just the model itself? I think the \"\n",
      " 'model needs specific parameters. Maybe something about temperature and '\n",
      " 'top-k? Those are common hyperparameters in models like GPT-3. So if someone '\n",
      " 'wants to set a temperature of 0.5 and only get the top 10 most probable '\n",
      " 'responses, they need to configure the model accordingly.\\n'\n",
      " '\\n'\n",
      " 'Also, how does it handle context? I guess it looks at the surrounding text '\n",
      " 'to generate more meaningful answers. That makes sense because context can '\n",
      " 'provide better depth and coherence in responses.\\n'\n",
      " '\\n'\n",
      " 'I wonder about evaluation. How do you measure the quality of a LangChain '\n",
      " 'response? Maybe through metrics like BLEU or ROUGE if the model is used for '\n",
      " \"translation or summarization tasks. But for general purposes, it's often \"\n",
      " 'just how well it produces the desired text.\\n'\n",
      " '\\n'\n",
      " \"Another point: it's an open framework. That means different people can build \"\n",
      " 'upon it with custom modules and models. It encourages experimentation '\n",
      " 'without strict guidelines, which can lead to innovation but also requires '\n",
      " 'careful setup.\\n'\n",
      " '\\n'\n",
      " 'I should consider how LangChain compares to other NLP tools like ChatGPT or '\n",
      " \"OpenAI's API. While similar in the goal of generating responses, the \"\n",
      " 'specifics differ, especially in terms of integration with various platforms '\n",
      " 'or custom components.\\n'\n",
      " '\\n'\n",
      " 'What about user training? If someone uses LangChain, do they need additional '\n",
      " \"training? Probably, as it's built on top of models that require parameter \"\n",
      " 'tuning and understanding of context.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain is a framework for text generation using '\n",
      " 'transformer-based models. It involves configuring the model with parameters '\n",
      " 'like temperature and top-k, allowing it to generate responses based on '\n",
      " \"context. It's widely used across various applications, but its effectiveness \"\n",
      " 'depends on the specific use case and customization.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain, also known as the Language Generation Chain, is a comprehensive '\n",
      " 'framework designed for text generation using transformer-based models. '\n",
      " \"Here's an organized summary of its key components and features:\\n\"\n",
      " '\\n'\n",
      " '1. **Concept**: LangChain operates on the transformer architecture, which '\n",
      " 'excels in natural language processing tasks such as language modeling, '\n",
      " 'attention mechanisms, and classification.\\n'\n",
      " '\\n'\n",
      " '2. **Model Configuration**: \\n'\n",
      " '   - **Temperature**: Controls the diversity of responses by adjusting how '\n",
      " 'certain outputs are weighted.\\n'\n",
      " '   - **Top-k**: Limiting the number of most probable responses generated.\\n'\n",
      " '\\n'\n",
      " '3. **Context Handling**: Language models in LangChain analyze surrounding '\n",
      " 'text to produce more meaningful and coherent responses, enhancing depth and '\n",
      " 'context.\\n'\n",
      " '\\n'\n",
      " '4. **Evaluation Metrics**: While effectiveness depends on specific use '\n",
      " 'cases, common metrics like BLEU or ROUGE can assess translation quality. For '\n",
      " 'general purposes, response clarity is key.\\n'\n",
      " '\\n'\n",
      " \"5. **Open Framework**: LangChain's open architecture allows customization \"\n",
      " 'through module integration, fostering innovation without strict guidelines, '\n",
      " 'encouraging experimentation.\\n'\n",
      " '\\n'\n",
      " \"6. **Comparison with Other Tools**: Similar to ChatGPT and OpenAI's API but \"\n",
      " 'diverges in model specifics and context handling. Integration varies across '\n",
      " 'platforms or custom components.\\n'\n",
      " '\\n'\n",
      " '7. **Training Needs**: Requires parameter tuning and understanding of '\n",
      " 'context for effective use.\\n'\n",
      " '\\n'\n",
      " '8. **User Training**: Users need additional training, as it involves '\n",
      " 'mastering transformer configurations and context awareness.\\n'\n",
      " '\\n'\n",
      " 'In essence, LangChain is a versatile framework enabling text generation '\n",
      " 'through transformers, widely applicable in various applications but '\n",
      " 'requiring careful setup and customization.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ deepseek-r1:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and versatile. I should mention that it was created by Guido van Rossum in 1991. \n",
      "\n",
      "Wait, the user might be a beginner, so I should explain it in simple terms. Maybe start with the key points: it's a high-level language, interpreted, open-source, and used for web development, data analysis, automation, etc. Also, mention that it's widely used in various fields like AI, machine learning, and scientific computing.\n",
      "\n",
      "I should check if there are any important features or benefits to highlight. Oh, right, the \"Pythonic\" way of writing code, the extensive library ecosystem, and how it's beginner-friendly. Maybe include some examples, like how it's used in projects or popular frameworks.\n",
      "\n",
      "Wait, the user might not know the full scope, so I should balance between the basics and some applications. Avoid technical jargon unless necessary. Also, make sure the answer is clear and not too lengthy. Let me structure it with a definition, key features, and applications. Check for any typos or mistakes. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ê³ ê¸‰ ìˆ˜ì¤€ì˜ ì½”ë“œë¥¼ ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆëŠ” **ê°„ê²°í•˜ê³  ì§ê´€ì ì¸ ì–¸ì–´**ì…ë‹ˆë‹¤. 1991ë…„ì— í”„ë¡œì íŠ¸ íŒ€ìœ¼ë¡œë¶€í„° Guido van Rossumì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**íŠ¹ì§•**:  \n",
      "1. **ê°„ê²°í•œ ë¬¸ë²•**: ë³µì¡í•œ êµ¬ë¬¸ì´ ì—†ì–´ ì‹ ì… ê°œë°œìë„ ì‰½ê²Œ ë°°ì›Œì§‘ë‹ˆë‹¤.  \n",
      "2. **ê³ ì† ì‹¤í–‰**: ì½”ë“œ ì‹¤í–‰ ì†ë„ê°€ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í”„ë¡œê·¸ë˜ë°ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.  \n",
      "3. **ë‹¤ì–‘í•œ í™œìš© ë¶„ì•¼**: ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, ìë™í™”, AI, ë¨¸ì‹ ëŸ¬ë‹, ê³¼í•™ ê³„ì‚° ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
      "4. **ê°œë°©ì  ì†Œí”„íŠ¸ì›¨ì–´**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µë˜ì–´ ëˆ„êµ¬ë‚˜ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**ì‚¬ìš© ì˜ˆ**:  \n",
      "- ì›¹ì‚¬ì´íŠ¸ ê°œë°œ (Django, Flask)  \n",
      "- ë°ì´í„° ë¶„ì„ (Pandas, NumPy)  \n",
      "- ìë™í™” (ì˜ˆ: ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±)  \n",
      "- AI ë° ë¨¸ì‹ ëŸ¬ë‹ (TensorFlow, PyTorch)  \n",
      "\n",
      "íŒŒì´ì¬ì€ **\"Pythonic\"** ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆì–´, í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ qwen2.5:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? í•œê¸€ë¡œ ë‹µë³€í•´ ì¤˜\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger, I'll compare the whole numbers first.\n",
      "\n",
      "Both numbers have a whole number part of 9, so they are equal in that regard.\n",
      "\n",
      "Next, I'll examine the decimal parts:\n",
      "- The tenths place: 9.9 has a 9, while 9.11 has a 1.\n",
      "Since 9 is greater than 1, 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Compare the Whole Numbers\n",
      "Both numbers have the same whole number part:\n",
      "- **9**\n",
      "\n",
      "Since they are equal, we move to the decimal parts.\n",
      "\n",
      "### Step 2: Compare the Tenths Place\n",
      "- **9.9** has a **9** in the tenths place.\n",
      "- **9.11** has a **1** in the tenths place.\n",
      "\n",
      "**Conclusion:**  \n",
      "The tenths place of **9.9** (which is **9**) is greater than that of **9.11** (which is **1**).\n",
      "\n",
      "### Final Answer\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174fec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger, I'll compare the whole numbers first.\n",
       "\n",
       "Both numbers have a whole number part of 9, so they are equal in that regard.\n",
       "\n",
       "Next, I'll examine the decimal parts:\n",
       "- The tenths place: 9.9 has a 9, while 9.11 has a 1.\n",
       "Since 9 is greater than 1, 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Numbers\n",
       "Both numbers have the same whole number part:\n",
       "- **9**\n",
       "\n",
       "Since they are equal, we move to the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Tenths Place\n",
       "- **9.9** has a **9** in the tenths place.\n",
       "- **9.11** has a **1** in the tenths place.\n",
       "\n",
       "**Conclusion:**  \n",
       "The tenths place of **9.9** (which is **9**) is greater than that of **9.11** (which is **1**).\n",
       "\n",
       "### Final Answer\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
      "\n",
      "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
      "\n",
      "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So even though the whole numbers are the same, the decimal part of 9.9 is bigger. Therefore, 9.9 is larger than 9.11. \n",
      "\n",
      "But let me double-check to make sure I didn't make a mistake. Sometimes when comparing decimals, people might confuse the places. Let's write them out:\n",
      "\n",
      "9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place first: 9 vs 1. Since 9 is larger than 1, 9.90 is larger than 9.11. Yeah, that makes sense. So even if the second number has more decimal places, the tenths place is already different. \n",
      "\n",
      "Another way to think about it: 9.9 is 9.90, and 9.11 is 9.11. If you line them up:\n",
      "\n",
      "9.90\n",
      "9.11\n",
      "\n",
      "Comparing digit by digit from the left:\n",
      "\n",
      "- The first digit after the decimal is 9 vs 1. Since 9 is greater than 1, the first number is larger. \n",
      "\n",
      "So definitely, 9.9 is bigger than 9.11. I don't think there's any other way to interpret this. The whole numbers are the same, but the decimal parts are different. The decimal part of 9.9 is larger, so the entire number is larger. \n",
      "\n",
      "I guess that's it. The answer should be 9.9.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ˜ëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ì´ìœ :**  \n",
      "- ë‘ ìˆ˜ ëª¨ë‘ 9ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.  \n",
      "- 9.9ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.9, 9.11ì˜ ì†Œìˆ˜ ë¶€ë¶„ì€ 0.11ì…ë‹ˆë‹¤.  \n",
      "- 0.9ëŠ” 0.11ë³´ë‹¤ í¬ë¯€ë¡œ, 9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "\n",
      "ë”°ë¼ì„œ, **9.9**ì´ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "#model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3623342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4cdc8",
   "metadata": {},
   "source": [
    "### LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ DeepSeek ëª¨ë¸(ì¶”ë¡ )ê³¼ Qwen ëª¨ë¸(í•œê¸€ì‘ë‹µ)ì„ ì—°ë™í•˜ê¸°\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f146748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.7)\n",
    "print(generation_model)\n",
    "\n",
    "#LangGraphì—ì„œ State ì‚¬ìš©ìì •ì˜ í´ë˜ìŠ¤ëŠ” ë…¸ë“œ ê°„ì˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” í‹€ì…ë‹ˆë‹¤. \n",
    "#ë…¸ë“œ ê°„ì— ê³„ì† ì „ë‹¬í•˜ê³  ì‹¶ê±°ë‚˜, ê·¸ë˜í”„ ë‚´ì—ì„œ ìœ ì§€í•´ì•¼ í•  ì •ë³´ë¥¼ ë¯¸ë¦¬ ì •ì˜í™ë‹ˆë‹¤. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸: {question}\n",
    "        ì¶”ë¡ : {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
