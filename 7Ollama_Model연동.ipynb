{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " 'Alright, so I need to figure out what LangChain is. The user already '\n",
      " 'provided a pretty comprehensive breakdown, but maybe I can think through it '\n",
      " 'step by step.\\n'\n",
      " '\\n'\n",
      " \"First, I know that LangChain stands for Language Generation Chain. It's part \"\n",
      " 'of the LangChain framework, which is used for generating text. But how '\n",
      " 'exactly does it work? I remember something about transformers being involved '\n",
      " 'in this process.\\n'\n",
      " '\\n'\n",
      " 'So, transformers are a big thing in NLP, right? They help with tasks like '\n",
      " 'language modeling, attention mechanisms, and classification. In the context '\n",
      " \"of LangChain, maybe they're used for generating coherent responses or \"\n",
      " 'instructions.\\n'\n",
      " '\\n'\n",
      " \"But wait, isn't there more to it than just the model itself? I think the \"\n",
      " 'model needs specific parameters. Maybe something about temperature and '\n",
      " 'top-k? Those are common hyperparameters in models like GPT-3. So if someone '\n",
      " 'wants to set a temperature of 0.5 and only get the top 10 most probable '\n",
      " 'responses, they need to configure the model accordingly.\\n'\n",
      " '\\n'\n",
      " 'Also, how does it handle context? I guess it looks at the surrounding text '\n",
      " 'to generate more meaningful answers. That makes sense because context can '\n",
      " 'provide better depth and coherence in responses.\\n'\n",
      " '\\n'\n",
      " 'I wonder about evaluation. How do you measure the quality of a LangChain '\n",
      " 'response? Maybe through metrics like BLEU or ROUGE if the model is used for '\n",
      " \"translation or summarization tasks. But for general purposes, it's often \"\n",
      " 'just how well it produces the desired text.\\n'\n",
      " '\\n'\n",
      " \"Another point: it's an open framework. That means different people can build \"\n",
      " 'upon it with custom modules and models. It encourages experimentation '\n",
      " 'without strict guidelines, which can lead to innovation but also requires '\n",
      " 'careful setup.\\n'\n",
      " '\\n'\n",
      " 'I should consider how LangChain compares to other NLP tools like ChatGPT or '\n",
      " \"OpenAI's API. While similar in the goal of generating responses, the \"\n",
      " 'specifics differ, especially in terms of integration with various platforms '\n",
      " 'or custom components.\\n'\n",
      " '\\n'\n",
      " 'What about user training? If someone uses LangChain, do they need additional '\n",
      " \"training? Probably, as it's built on top of models that require parameter \"\n",
      " 'tuning and understanding of context.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain is a framework for text generation using '\n",
      " 'transformer-based models. It involves configuring the model with parameters '\n",
      " 'like temperature and top-k, allowing it to generate responses based on '\n",
      " \"context. It's widely used across various applications, but its effectiveness \"\n",
      " 'depends on the specific use case and customization.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain, also known as the Language Generation Chain, is a comprehensive '\n",
      " 'framework designed for text generation using transformer-based models. '\n",
      " \"Here's an organized summary of its key components and features:\\n\"\n",
      " '\\n'\n",
      " '1. **Concept**: LangChain operates on the transformer architecture, which '\n",
      " 'excels in natural language processing tasks such as language modeling, '\n",
      " 'attention mechanisms, and classification.\\n'\n",
      " '\\n'\n",
      " '2. **Model Configuration**: \\n'\n",
      " '   - **Temperature**: Controls the diversity of responses by adjusting how '\n",
      " 'certain outputs are weighted.\\n'\n",
      " '   - **Top-k**: Limiting the number of most probable responses generated.\\n'\n",
      " '\\n'\n",
      " '3. **Context Handling**: Language models in LangChain analyze surrounding '\n",
      " 'text to produce more meaningful and coherent responses, enhancing depth and '\n",
      " 'context.\\n'\n",
      " '\\n'\n",
      " '4. **Evaluation Metrics**: While effectiveness depends on specific use '\n",
      " 'cases, common metrics like BLEU or ROUGE can assess translation quality. For '\n",
      " 'general purposes, response clarity is key.\\n'\n",
      " '\\n'\n",
      " \"5. **Open Framework**: LangChain's open architecture allows customization \"\n",
      " 'through module integration, fostering innovation without strict guidelines, '\n",
      " 'encouraging experimentation.\\n'\n",
      " '\\n'\n",
      " \"6. **Comparison with Other Tools**: Similar to ChatGPT and OpenAI's API but \"\n",
      " 'diverges in model specifics and context handling. Integration varies across '\n",
      " 'platforms or custom components.\\n'\n",
      " '\\n'\n",
      " '7. **Training Needs**: Requires parameter tuning and understanding of '\n",
      " 'context for effective use.\\n'\n",
      " '\\n'\n",
      " '8. **User Training**: Users need additional training, as it involves '\n",
      " 'mastering transformer configurations and context awareness.\\n'\n",
      " '\\n'\n",
      " 'In essence, LangChain is a versatile framework enabling text generation '\n",
      " 'through transformers, widely applicable in various applications but '\n",
      " 'requiring careful setup and customization.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ deepseek-r1:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and versatile. I should mention that it was created by Guido van Rossum in 1991. \n",
      "\n",
      "Wait, the user might be a beginner, so I should explain it in simple terms. Maybe start with the key points: it's a high-level language, interpreted, open-source, and used for web development, data analysis, automation, etc. Also, mention that it's widely used in various fields like AI, machine learning, and scientific computing.\n",
      "\n",
      "I should check if there are any important features or benefits to highlight. Oh, right, the \"Pythonic\" way of writing code, the extensive library ecosystem, and how it's beginner-friendly. Maybe include some examples, like how it's used in projects or popular frameworks.\n",
      "\n",
      "Wait, the user might not know the full scope, so I should balance between the basics and some applications. Avoid technical jargon unless necessary. Also, make sure the answer is clear and not too lengthy. Let me structure it with a definition, key features, and applications. Check for any typos or mistakes. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ê³ ê¸‰ ìˆ˜ì¤€ì˜ ì½”ë“œë¥¼ ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆëŠ” **ê°„ê²°í•˜ê³  ì§ê´€ì ì¸ ì–¸ì–´**ì…ë‹ˆë‹¤. 1991ë…„ì— í”„ë¡œì íŠ¸ íŒ€ìœ¼ë¡œë¶€í„° Guido van Rossumì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**íŠ¹ì§•**:  \n",
      "1. **ê°„ê²°í•œ ë¬¸ë²•**: ë³µì¡í•œ êµ¬ë¬¸ì´ ì—†ì–´ ì‹ ì… ê°œë°œìë„ ì‰½ê²Œ ë°°ì›Œì§‘ë‹ˆë‹¤.  \n",
      "2. **ê³ ì† ì‹¤í–‰**: ì½”ë“œ ì‹¤í–‰ ì†ë„ê°€ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í”„ë¡œê·¸ë˜ë°ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.  \n",
      "3. **ë‹¤ì–‘í•œ í™œìš© ë¶„ì•¼**: ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, ìë™í™”, AI, ë¨¸ì‹ ëŸ¬ë‹, ê³¼í•™ ê³„ì‚° ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
      "4. **ê°œë°©ì  ì†Œí”„íŠ¸ì›¨ì–´**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µë˜ì–´ ëˆ„êµ¬ë‚˜ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "**ì‚¬ìš© ì˜ˆ**:  \n",
      "- ì›¹ì‚¬ì´íŠ¸ ê°œë°œ (Django, Flask)  \n",
      "- ë°ì´í„° ë¶„ì„ (Pandas, NumPy)  \n",
      "- ìë™í™” (ì˜ˆ: ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±)  \n",
      "- AI ë° ë¨¸ì‹ ëŸ¬ë‹ (TensorFlow, PyTorch)  \n",
      "\n",
      "íŒŒì´ì¬ì€ **\"Pythonic\"** ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆì–´, í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ qwen2.5:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? í•œê¸€ë¡œ ë‹µë³€í•´ ì¤˜\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
