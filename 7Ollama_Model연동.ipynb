{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " 'Alright, so I need to figure out what LangChain is. The user already '\n",
      " 'provided a pretty comprehensive breakdown, but maybe I can think through it '\n",
      " 'step by step.\\n'\n",
      " '\\n'\n",
      " \"First, I know that LangChain stands for Language Generation Chain. It's part \"\n",
      " 'of the LangChain framework, which is used for generating text. But how '\n",
      " 'exactly does it work? I remember something about transformers being involved '\n",
      " 'in this process.\\n'\n",
      " '\\n'\n",
      " 'So, transformers are a big thing in NLP, right? They help with tasks like '\n",
      " 'language modeling, attention mechanisms, and classification. In the context '\n",
      " \"of LangChain, maybe they're used for generating coherent responses or \"\n",
      " 'instructions.\\n'\n",
      " '\\n'\n",
      " \"But wait, isn't there more to it than just the model itself? I think the \"\n",
      " 'model needs specific parameters. Maybe something about temperature and '\n",
      " 'top-k? Those are common hyperparameters in models like GPT-3. So if someone '\n",
      " 'wants to set a temperature of 0.5 and only get the top 10 most probable '\n",
      " 'responses, they need to configure the model accordingly.\\n'\n",
      " '\\n'\n",
      " 'Also, how does it handle context? I guess it looks at the surrounding text '\n",
      " 'to generate more meaningful answers. That makes sense because context can '\n",
      " 'provide better depth and coherence in responses.\\n'\n",
      " '\\n'\n",
      " 'I wonder about evaluation. How do you measure the quality of a LangChain '\n",
      " 'response? Maybe through metrics like BLEU or ROUGE if the model is used for '\n",
      " \"translation or summarization tasks. But for general purposes, it's often \"\n",
      " 'just how well it produces the desired text.\\n'\n",
      " '\\n'\n",
      " \"Another point: it's an open framework. That means different people can build \"\n",
      " 'upon it with custom modules and models. It encourages experimentation '\n",
      " 'without strict guidelines, which can lead to innovation but also requires '\n",
      " 'careful setup.\\n'\n",
      " '\\n'\n",
      " 'I should consider how LangChain compares to other NLP tools like ChatGPT or '\n",
      " \"OpenAI's API. While similar in the goal of generating responses, the \"\n",
      " 'specifics differ, especially in terms of integration with various platforms '\n",
      " 'or custom components.\\n'\n",
      " '\\n'\n",
      " 'What about user training? If someone uses LangChain, do they need additional '\n",
      " \"training? Probably, as it's built on top of models that require parameter \"\n",
      " 'tuning and understanding of context.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain is a framework for text generation using '\n",
      " 'transformer-based models. It involves configuring the model with parameters '\n",
      " 'like temperature and top-k, allowing it to generate responses based on '\n",
      " \"context. It's widely used across various applications, but its effectiveness \"\n",
      " 'depends on the specific use case and customization.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain, also known as the Language Generation Chain, is a comprehensive '\n",
      " 'framework designed for text generation using transformer-based models. '\n",
      " \"Here's an organized summary of its key components and features:\\n\"\n",
      " '\\n'\n",
      " '1. **Concept**: LangChain operates on the transformer architecture, which '\n",
      " 'excels in natural language processing tasks such as language modeling, '\n",
      " 'attention mechanisms, and classification.\\n'\n",
      " '\\n'\n",
      " '2. **Model Configuration**: \\n'\n",
      " '   - **Temperature**: Controls the diversity of responses by adjusting how '\n",
      " 'certain outputs are weighted.\\n'\n",
      " '   - **Top-k**: Limiting the number of most probable responses generated.\\n'\n",
      " '\\n'\n",
      " '3. **Context Handling**: Language models in LangChain analyze surrounding '\n",
      " 'text to produce more meaningful and coherent responses, enhancing depth and '\n",
      " 'context.\\n'\n",
      " '\\n'\n",
      " '4. **Evaluation Metrics**: While effectiveness depends on specific use '\n",
      " 'cases, common metrics like BLEU or ROUGE can assess translation quality. For '\n",
      " 'general purposes, response clarity is key.\\n'\n",
      " '\\n'\n",
      " \"5. **Open Framework**: LangChain's open architecture allows customization \"\n",
      " 'through module integration, fostering innovation without strict guidelines, '\n",
      " 'encouraging experimentation.\\n'\n",
      " '\\n'\n",
      " \"6. **Comparison with Other Tools**: Similar to ChatGPT and OpenAI's API but \"\n",
      " 'diverges in model specifics and context handling. Integration varies across '\n",
      " 'platforms or custom components.\\n'\n",
      " '\\n'\n",
      " '7. **Training Needs**: Requires parameter tuning and understanding of '\n",
      " 'context for effective use.\\n'\n",
      " '\\n'\n",
      " '8. **User Training**: Users need additional training, as it involves '\n",
      " 'mastering transformer configurations and context awareness.\\n'\n",
      " '\\n'\n",
      " 'In essence, LangChain is a versatile framework enabling text generation '\n",
      " 'through transformers, widely applicable in various applications but '\n",
      " 'requiring careful setup and customization.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 deepseek-r1:1.5b 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and versatile. I should mention that it was created by Guido van Rossum in 1991. \n",
      "\n",
      "Wait, the user might be a beginner, so I should explain it in simple terms. Maybe start with the key points: it's a high-level language, interpreted, open-source, and used for web development, data analysis, automation, etc. Also, mention that it's widely used in various fields like AI, machine learning, and scientific computing.\n",
      "\n",
      "I should check if there are any important features or benefits to highlight. Oh, right, the \"Pythonic\" way of writing code, the extensive library ecosystem, and how it's beginner-friendly. Maybe include some examples, like how it's used in projects or popular frameworks.\n",
      "\n",
      "Wait, the user might not know the full scope, so I should balance between the basics and some applications. Avoid technical jargon unless necessary. Also, make sure the answer is clear and not too lengthy. Let me structure it with a definition, key features, and applications. Check for any typos or mistakes. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어로, 고급 수준의 코드를 쉽게 작성할 수 있는 **간결하고 직관적인 언어**입니다. 1991년에 프로젝트 팀으로부터 Guido van Rossum에 의해 개발되었습니다.  \n",
      "\n",
      "**특징**:  \n",
      "1. **간결한 문법**: 복잡한 구문이 없어 신입 개발자도 쉽게 배워집니다.  \n",
      "2. **고속 실행**: 코드 실행 속도가 빠르고 효율적인 프로그래밍을 가능하게 합니다.  \n",
      "3. **다양한 활용 분야**: 웹 개발, 데이터 분석, 자동화, AI, 머신러닝, 과학 계산 등 다양한 분야에서 널리 사용됩니다.  \n",
      "4. **개방적 소프트웨어**: 오픈소스로 제공되어 누구나 자유롭게 사용할 수 있습니다.  \n",
      "\n",
      "**사용 예**:  \n",
      "- 웹사이트 개발 (Django, Flask)  \n",
      "- 데이터 분석 (Pandas, NumPy)  \n",
      "- 자동화 (예: 스크립트 작성)  \n",
      "- AI 및 머신러닝 (TensorFlow, PyTorch)  \n",
      "\n",
      "파이썬은 **\"Pythonic\"** 방식으로 코드를 작성할 수 있어, 편리하게 사용할 수 있습니다. 🐍\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 qwen2.5:1.5b 모델 로드\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요? 한글로 답변해 줘\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger, I'll compare the whole numbers first.\n",
      "\n",
      "Both numbers have a whole number part of 9, so they are equal in that regard.\n",
      "\n",
      "Next, I'll examine the decimal parts:\n",
      "- The tenths place: 9.9 has a 9, while 9.11 has a 1.\n",
      "Since 9 is greater than 1, 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Compare the Whole Numbers\n",
      "Both numbers have the same whole number part:\n",
      "- **9**\n",
      "\n",
      "Since they are equal, we move to the decimal parts.\n",
      "\n",
      "### Step 2: Compare the Tenths Place\n",
      "- **9.9** has a **9** in the tenths place.\n",
      "- **9.11** has a **1** in the tenths place.\n",
      "\n",
      "**Conclusion:**  \n",
      "The tenths place of **9.9** (which is **9**) is greater than that of **9.11** (which is **1**).\n",
      "\n",
      "### Final Answer\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174fec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger, I'll compare the whole numbers first.\n",
       "\n",
       "Both numbers have a whole number part of 9, so they are equal in that regard.\n",
       "\n",
       "Next, I'll examine the decimal parts:\n",
       "- The tenths place: 9.9 has a 9, while 9.11 has a 1.\n",
       "Since 9 is greater than 1, 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Compare the Whole Numbers\n",
       "Both numbers have the same whole number part:\n",
       "- **9**\n",
       "\n",
       "Since they are equal, we move to the decimal parts.\n",
       "\n",
       "### Step 2: Compare the Tenths Place\n",
       "- **9.9** has a **9** in the tenths place.\n",
       "- **9.11** has a **1** in the tenths place.\n",
       "\n",
       "**Conclusion:**  \n",
       "The tenths place of **9.9** (which is **9**) is greater than that of **9.11** (which is **1**).\n",
       "\n",
       "### Final Answer\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is larger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
      "\n",
      "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
      "\n",
      "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So even though the whole numbers are the same, the decimal part of 9.9 is bigger. Therefore, 9.9 is larger than 9.11. \n",
      "\n",
      "But let me double-check to make sure I didn't make a mistake. Sometimes when comparing decimals, people might confuse the places. Let's write them out:\n",
      "\n",
      "9.9 is 9.90, and 9.11 is 9.11. Comparing the tenths place first: 9 vs 1. Since 9 is larger than 1, 9.90 is larger than 9.11. Yeah, that makes sense. So even if the second number has more decimal places, the tenths place is already different. \n",
      "\n",
      "Another way to think about it: 9.9 is 9.90, and 9.11 is 9.11. If you line them up:\n",
      "\n",
      "9.90\n",
      "9.11\n",
      "\n",
      "Comparing digit by digit from the left:\n",
      "\n",
      "- The first digit after the decimal is 9 vs 1. Since 9 is greater than 1, the first number is larger. \n",
      "\n",
      "So definitely, 9.9 is bigger than 9.11. I don't think there's any other way to interpret this. The whole numbers are the same, but the decimal parts are different. The decimal part of 9.9 is larger, so the entire number is larger. \n",
      "\n",
      "I guess that's it. The answer should be 9.9.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "\n",
      "**이유:**  \n",
      "- 두 수 모두 9를 기준으로 시작합니다.  \n",
      "- 9.9의 소수 부분은 0.9, 9.11의 소수 부분은 0.11입니다.  \n",
      "- 0.9는 0.11보다 크므로, 9.9는 9.11보다 더 큰 수입니다.  \n",
      "\n",
      "따라서, **9.9**이 더 큰 수입니다."
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "#model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3623342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4cdc8",
   "metadata": {},
   "source": [
    "### LangGraph를 사용하여 DeepSeek 모델(추론)과 Qwen 모델(한글응답)을 연동하기\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f146748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "generation_model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.7)\n",
    "print(generation_model)\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
