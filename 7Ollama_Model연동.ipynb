{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744867b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9eda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " 'Alright, so I need to figure out what LangChain is. The user already '\n",
      " 'provided a pretty comprehensive breakdown, but maybe I can think through it '\n",
      " 'step by step.\\n'\n",
      " '\\n'\n",
      " \"First, I know that LangChain stands for Language Generation Chain. It's part \"\n",
      " 'of the LangChain framework, which is used for generating text. But how '\n",
      " 'exactly does it work? I remember something about transformers being involved '\n",
      " 'in this process.\\n'\n",
      " '\\n'\n",
      " 'So, transformers are a big thing in NLP, right? They help with tasks like '\n",
      " 'language modeling, attention mechanisms, and classification. In the context '\n",
      " \"of LangChain, maybe they're used for generating coherent responses or \"\n",
      " 'instructions.\\n'\n",
      " '\\n'\n",
      " \"But wait, isn't there more to it than just the model itself? I think the \"\n",
      " 'model needs specific parameters. Maybe something about temperature and '\n",
      " 'top-k? Those are common hyperparameters in models like GPT-3. So if someone '\n",
      " 'wants to set a temperature of 0.5 and only get the top 10 most probable '\n",
      " 'responses, they need to configure the model accordingly.\\n'\n",
      " '\\n'\n",
      " 'Also, how does it handle context? I guess it looks at the surrounding text '\n",
      " 'to generate more meaningful answers. That makes sense because context can '\n",
      " 'provide better depth and coherence in responses.\\n'\n",
      " '\\n'\n",
      " 'I wonder about evaluation. How do you measure the quality of a LangChain '\n",
      " 'response? Maybe through metrics like BLEU or ROUGE if the model is used for '\n",
      " \"translation or summarization tasks. But for general purposes, it's often \"\n",
      " 'just how well it produces the desired text.\\n'\n",
      " '\\n'\n",
      " \"Another point: it's an open framework. That means different people can build \"\n",
      " 'upon it with custom modules and models. It encourages experimentation '\n",
      " 'without strict guidelines, which can lead to innovation but also requires '\n",
      " 'careful setup.\\n'\n",
      " '\\n'\n",
      " 'I should consider how LangChain compares to other NLP tools like ChatGPT or '\n",
      " \"OpenAI's API. While similar in the goal of generating responses, the \"\n",
      " 'specifics differ, especially in terms of integration with various platforms '\n",
      " 'or custom components.\\n'\n",
      " '\\n'\n",
      " 'What about user training? If someone uses LangChain, do they need additional '\n",
      " \"training? Probably, as it's built on top of models that require parameter \"\n",
      " 'tuning and understanding of context.\\n'\n",
      " '\\n'\n",
      " 'In summary, LangChain is a framework for text generation using '\n",
      " 'transformer-based models. It involves configuring the model with parameters '\n",
      " 'like temperature and top-k, allowing it to generate responses based on '\n",
      " \"context. It's widely used across various applications, but its effectiveness \"\n",
      " 'depends on the specific use case and customization.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain, also known as the Language Generation Chain, is a comprehensive '\n",
      " 'framework designed for text generation using transformer-based models. '\n",
      " \"Here's an organized summary of its key components and features:\\n\"\n",
      " '\\n'\n",
      " '1. **Concept**: LangChain operates on the transformer architecture, which '\n",
      " 'excels in natural language processing tasks such as language modeling, '\n",
      " 'attention mechanisms, and classification.\\n'\n",
      " '\\n'\n",
      " '2. **Model Configuration**: \\n'\n",
      " '   - **Temperature**: Controls the diversity of responses by adjusting how '\n",
      " 'certain outputs are weighted.\\n'\n",
      " '   - **Top-k**: Limiting the number of most probable responses generated.\\n'\n",
      " '\\n'\n",
      " '3. **Context Handling**: Language models in LangChain analyze surrounding '\n",
      " 'text to produce more meaningful and coherent responses, enhancing depth and '\n",
      " 'context.\\n'\n",
      " '\\n'\n",
      " '4. **Evaluation Metrics**: While effectiveness depends on specific use '\n",
      " 'cases, common metrics like BLEU or ROUGE can assess translation quality. For '\n",
      " 'general purposes, response clarity is key.\\n'\n",
      " '\\n'\n",
      " \"5. **Open Framework**: LangChain's open architecture allows customization \"\n",
      " 'through module integration, fostering innovation without strict guidelines, '\n",
      " 'encouraging experimentation.\\n'\n",
      " '\\n'\n",
      " \"6. **Comparison with Other Tools**: Similar to ChatGPT and OpenAI's API but \"\n",
      " 'diverges in model specifics and context handling. Integration varies across '\n",
      " 'platforms or custom components.\\n'\n",
      " '\\n'\n",
      " '7. **Training Needs**: Requires parameter tuning and understanding of '\n",
      " 'context for effective use.\\n'\n",
      " '\\n'\n",
      " '8. **User Training**: Users need additional training, as it involves '\n",
      " 'mastering transformer configurations and context awareness.\\n'\n",
      " '\\n'\n",
      " 'In essence, LangChain is a versatile framework enabling text generation '\n",
      " 'through transformers, widely applicable in various applications but '\n",
      " 'requiring careful setup and customization.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 deepseek-r1:1.5b 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0860666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, and they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and versatile. I should mention that it was created by Guido van Rossum in 1991. \n",
      "\n",
      "Wait, the user might be a beginner, so I should explain it in simple terms. Maybe start with the key points: it's a high-level language, interpreted, open-source, and used for web development, data analysis, automation, etc. Also, mention that it's widely used in various fields like AI, machine learning, and scientific computing.\n",
      "\n",
      "I should check if there are any important features or benefits to highlight. Oh, right, the \"Pythonic\" way of writing code, the extensive library ecosystem, and how it's beginner-friendly. Maybe include some examples, like how it's used in projects or popular frameworks.\n",
      "\n",
      "Wait, the user might not know the full scope, so I should balance between the basics and some applications. Avoid technical jargon unless necessary. Also, make sure the answer is clear and not too lengthy. Let me structure it with a definition, key features, and applications. Check for any typos or mistakes. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어로, 고급 수준의 코드를 쉽게 작성할 수 있는 **간결하고 직관적인 언어**입니다. 1991년에 프로젝트 팀으로부터 Guido van Rossum에 의해 개발되었습니다.  \n",
      "\n",
      "**특징**:  \n",
      "1. **간결한 문법**: 복잡한 구문이 없어 신입 개발자도 쉽게 배워집니다.  \n",
      "2. **고속 실행**: 코드 실행 속도가 빠르고 효율적인 프로그래밍을 가능하게 합니다.  \n",
      "3. **다양한 활용 분야**: 웹 개발, 데이터 분석, 자동화, AI, 머신러닝, 과학 계산 등 다양한 분야에서 널리 사용됩니다.  \n",
      "4. **개방적 소프트웨어**: 오픈소스로 제공되어 누구나 자유롭게 사용할 수 있습니다.  \n",
      "\n",
      "**사용 예**:  \n",
      "- 웹사이트 개발 (Django, Flask)  \n",
      "- 데이터 분석 (Pandas, NumPy)  \n",
      "- 자동화 (예: 스크립트 작성)  \n",
      "- AI 및 머신러닝 (TensorFlow, PyTorch)  \n",
      "\n",
      "파이썬은 **\"Pythonic\"** 방식으로 코드를 작성할 수 있어, 편리하게 사용할 수 있습니다. 🐍\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 qwen2.5:1.5b 모델 로드\n",
    "#llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "#qwen3:1.7b\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요? 한글로 답변해 줘\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0af4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
