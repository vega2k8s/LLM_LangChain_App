{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94827d41",
   "metadata": {},
   "source": [
    "### 1. CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f103fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 원본 텍스트:\n",
      "--------------------------------------------------\n",
      "RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다. \n",
      "RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다.\n",
      "이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다.\n",
      "\n",
      " 원본 길이: 235자\n",
      "\n",
      "============================================================\n",
      " 다양한 CharacterTextSplitter 설정 비교\n",
      "============================================================\n",
      "\n",
      " 마침표(.) 기준 분할:\n",
      "------------------------------\n",
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다' (길이: 24자)\n",
      "청크 2: '기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다' (길이: 32자)\n",
      "청크 3: '특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다' (길이: 32자)\n",
      "청크 4: 'RAG는 검색과 생성 단계를 포함합니다' (길이: 21자)\n",
      "청크 5: '먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다' (길이: 43자)\n",
      "청크 6: '이 방식은 환상(hallucination) 문제를 크게 줄여줍니다' (길이: 36자)\n",
      "청크 7: '또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다' (길이: 33자)\n",
      "\n",
      " 문장 기준 분할 (큰 청크):\n",
      "------------------------------\n",
      "청크 1: 'RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다' (길이: 92자)\n",
      "청크 2: '특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다. \n",
      "RAG는 검색과 생성 단계를 포함합니다' (길이: 56자)\n",
      "청크 3: 'RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다' (길이: 66자)\n",
      "청크 4: '먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다.\n",
      "이 방식은 환상(hallucination) 문제를 크게 줄여줍니다' (길이: 81자)\n",
      "청크 5: '이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다' (길이: 71자)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# ===================================\n",
    "# 예제 텍스트\n",
    "# ===================================\n",
    "text = \"\"\"RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다. \n",
    "RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다.\n",
    "이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다.\"\"\"\n",
    "\n",
    "print(\" 원본 텍스트:\")\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "print(f\"\\n 원본 길이: {len(text)}자\")\n",
    "\n",
    "# ===================================\n",
    "#  다양한 분할 방식 비교\n",
    "# ===================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 다양한 CharacterTextSplitter 설정 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 기본 설정 (마침표 기준)\n",
    "print(\"\\n 마침표(.) 기준 분할:\")\n",
    "print(\"-\" * 30)\n",
    "splitter1 = CharacterTextSplitter(\n",
    "    chunk_size=50,      # 청크 최대 크기\n",
    "    chunk_overlap=10,   # 청크 간 중복\n",
    "    separator=\".\"       # 분할 기준\n",
    ")\n",
    "chunks1 = splitter1.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks1, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  문장 기준 (좀 더 큰 청크)\n",
    "print(\"\\n 문장 기준 분할 (큰 청크):\")\n",
    "print(\"-\" * 30)\n",
    "splitter2 = CharacterTextSplitter(\n",
    "    chunk_size=100,     # 더 큰 청크\n",
    "    chunk_overlap=50,   # 더 많은 중복\n",
    "    separator=\".\"\n",
    ")\n",
    "chunks2 = splitter2.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks2, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  줄바꿈 기준\n",
    "print(\"\\n 줄바꿈(\\\\n) 기준 분할:\")\n",
    "print(\"-\" * 30)\n",
    "splitter3 = CharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=0,    # 중복 없음\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "chunks3 = splitter3.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks3, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  공백 기준 (단어 단위)\n",
    "print(\"\\n 공백(' ') 기준 분할 (단어 단위):\")\n",
    "print(\"-\" * 30)\n",
    "splitter4 = CharacterTextSplitter(\n",
    "    chunk_size=30,      # 작은 청크\n",
    "    chunk_overlap=5,\n",
    "    separator=\" \"       # 공백으로 분할\n",
    ")\n",
    "chunks4 = splitter4.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks4[:5], 1):  # 처음 5개만 출력\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "print(f\"... 총 {len(chunks4)}개 청크 생성됨\")\n",
    "\n",
    "# ===================================\n",
    "#  설정별 결과 비교\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 설정별 결과 요약\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    (\"마침표 기준 (50자)\", len(chunks1), chunks1),\n",
    "    (\"마침표 기준 (100자)\", len(chunks2), chunks2),\n",
    "    (\"줄바꿈 기준\", len(chunks3), chunks3),\n",
    "    (\"공백 기준\", len(chunks4), chunks4)\n",
    "]\n",
    "\n",
    "for name, count, chunks in results:\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    print(f\"{name:15}: {count:2}개 청크, 평균 {avg_length:.1f}자\")\n",
    "\n",
    "# ===================================\n",
    "#  chunk_overlap 효과 확인\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" chunk_overlap 효과 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 중복 없음\n",
    "splitter_no_overlap = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=0, separator=\".\"\n",
    ")\n",
    "chunks_no_overlap = splitter_no_overlap.split_text(text)\n",
    "\n",
    "# 중복 있음\n",
    "splitter_with_overlap = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=15, separator=\".\"\n",
    ")\n",
    "chunks_with_overlap = splitter_with_overlap.split_text(text)\n",
    "\n",
    "print(\"\\n 중복 없음 (overlap=0):\")\n",
    "for i, chunk in enumerate(chunks_no_overlap, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}'\")\n",
    "\n",
    "print(\"\\n 중복 있음 (overlap=15):\")\n",
    "for i, chunk in enumerate(chunks_with_overlap, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}'\")\n",
    "    if i > 1:  # 두 번째 청크부터 중복 부분 표시\n",
    "        prev_chunk = chunks_with_overlap[i-2].strip()\n",
    "        curr_chunk = chunk.strip()\n",
    "        # 간단한 중복 확인\n",
    "        if len(prev_chunk) > 10 and len(curr_chunk) > 10:\n",
    "            if prev_chunk[-10:] in curr_chunk:\n",
    "                print(f\"    이전 청크와 중복: '{prev_chunk[-10:]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2fd28",
   "metadata": {},
   "source": [
    "### 2. RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a6db3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트:\n",
      "--------------------------------------------------\n",
      "RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
      "\n",
      "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.\n",
      "\n",
      "Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.\n",
      "\n",
      "RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다.\n",
      "\n",
      "텍스트 길이: 230자\n",
      "\n",
      "============================================================\n",
      "RecursiveCharacterTextSplitter vs CharacterTextSplitter 비교\n",
      "============================================================\n",
      "\n",
      "1. RecursiveCharacterTextSplitter (계층적 분할):\n",
      "---------------------------------------------\n",
      "Chunk 1: 'RAG는 검색과 생성 단계를 포함하는 모델입니다.'\n",
      "길이: 27자\n",
      "\n",
      "Chunk 2: '이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
      "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.'\n",
      "길이: 67자\n",
      "\n",
      "Chunk 3: 'Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.'\n",
      "길이: 72자\n",
      "\n",
      "Chunk 4: 'RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다.'\n",
      "길이: 58자\n",
      "\n",
      "2. CharacterTextSplitter (단순 분할):\n",
      "-----------------------------------\n",
      "Chunk 1: 'RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
      "\n",
      "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다'\n",
      "길이: 58자\n",
      "\n",
      "Chunk 2: '특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다'\n",
      "길이: 35자\n",
      "\n",
      "Chunk 3: 'Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다'\n",
      "길이: 71자\n",
      "\n",
      "Chunk 4: 'RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다'\n",
      "길이: 57자\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 예제 텍스트\n",
    "text = \"\"\"RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
    "\n",
    "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.\n",
    "\n",
    "Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.\n",
    "\n",
    "RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다.\"\"\"\n",
    "\n",
    "print(\"원본 텍스트:\")\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "print(f\"\\n텍스트 길이: {len(text)}자\")\n",
    "\n",
    "# ===========================================\n",
    "# Recursive vs Character 비교\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RecursiveCharacterTextSplitter vs CharacterTextSplitter 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. RecursiveCharacterTextSplitter (추천)\n",
    "print(\"\\n1. RecursiveCharacterTextSplitter (계층적 분할):\")\n",
    "print(\"-\" * 45)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]  # 우선순위 순서\n",
    ")\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i+1}: '{chunk.strip()}'\")\n",
    "    print(f\"길이: {len(chunk)}자\")\n",
    "    print()\n",
    "\n",
    "# 2. CharacterTextSplitter (비교용)\n",
    "print(\"2. CharacterTextSplitter (단순 분할):\")\n",
    "print(\"-\" * 35)\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separator=\".\"  # 하나의 구분자만 사용\n",
    ")\n",
    "simple_chunks = simple_splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(simple_chunks):\n",
    "    print(f\"Chunk {i+1}: '{chunk.strip()}'\")\n",
    "    print(f\"길이: {len(chunk)}자\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================================\n",
    "# separators 우선순위 테스트\n",
    "# ===========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"separators 우선순위 동작 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_text = \"\"\"첫 번째 문단입니다.\n",
    "\n",
    "두 번째 문단입니다.\n",
    "이 문단은 여러 문장으로 구성됩니다! 정말 흥미롭죠?\n",
    "\n",
    "세 번째 문단입니다.\"\"\"\n",
    "\n",
    "print(\"테스트 텍스트:\")\n",
    "print(repr(test_text))  # 줄바꿈 문자까지 보이도록\n",
    "\n",
    "# 다양한 separators 설정 테스트\n",
    "separators_configs = [\n",
    "    ([\"\\n\\n\", \"\\n\", \".\", \" \"], \"문단 우선\"),\n",
    "    ([\"\\n\", \".\", \" \"], \"줄바꿈 우선\"),\n",
    "    ([\".\", \"!\", \"?\", \" \"], \"문장 우선\"),\n",
    "    ([\" \"], \"단어 단위\")\n",
    "]\n",
    "\n",
    "for separators, description in separators_configs:\n",
    "    print(f\"\\n{description} separators={separators}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=40,\n",
    "        chunk_overlap=10,\n",
    "        separators=separators\n",
    "    )\n",
    "    chunks = splitter.split_text(test_text)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()}'\")\n",
    "\n",
    "# ===========================================\n",
    "# chunk_size별 결과 비교\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"chunk_size별 분할 결과 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chunk_sizes = [50, 100, 150]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={size}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=20,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    print(f\"총 {len(chunks)}개 청크 생성\")\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    print(f\"평균 청크 길이: {avg_length:.1f}자\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()[:30]}...' (길이: {len(chunk)}자)\")\n",
    "\n",
    "# ===========================================\n",
    "# chunk_overlap 효과 확인\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"chunk_overlap 효과 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overlap_values = [0, 10, 30]\n",
    "\n",
    "for overlap in overlap_values:\n",
    "    print(f\"\\nchunk_overlap={overlap}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=80,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    print(f\"총 {len(chunks)}개 청크 생성\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()}'\")\n",
    "        \n",
    "        # 중복 부분 확인\n",
    "        if i > 1 and overlap > 0:\n",
    "            prev_chunk = chunks[i-2].strip()\n",
    "            curr_chunk = chunk.strip()\n",
    "            # 간단한 중복 확인 (마지막 10자와 첫 10자 비교)\n",
    "            if len(prev_chunk) >= 10 and len(curr_chunk) >= 10:\n",
    "                prev_end = prev_chunk[-10:]\n",
    "                curr_start = curr_chunk[:10]\n",
    "                if any(word in curr_start for word in prev_end.split() if len(word) > 2):\n",
    "                    print(f\"    중복 감지: 이전 청크와 겹치는 부분 있음\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================================\n",
    "# 활용 가이드\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"실무 활용 가이드\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "RecursiveCharacterTextSplitter 사용 가이드:\n",
    "\n",
    "1. 기본 설정 (일반적 문서):\n",
    "   chunk_size=1000, chunk_overlap=200\n",
    "   separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "\n",
    "2. 한국어 문서 최적화:\n",
    "   chunk_size=500-1000, chunk_overlap=100-200\n",
    "   separators=[\"\\n\\n\", \"\\n\", \".\", \"。\", \" \"]\n",
    "\n",
    "3. 코드 문서:\n",
    "   separators=[\"\\n\\n\", \"\\n\", \"\\t\", \" \"]\n",
    "\n",
    "4. 대화/채팅 로그:\n",
    "   separators=[\"\\n\\n\", \"\\n\", \":\", \" \"]\n",
    "\n",
    "장점:\n",
    "- 의미 단위로 자연스러운 분할\n",
    "- 계층적 구분자로 최적화된 분할점 찾기\n",
    "- 텍스트 특성에 맞는 유연한 설정\n",
    "\n",
    "주의사항:\n",
    "- chunk_size는 LLM 토큰 제한 고려\n",
    "- chunk_overlap은 맥락 보존과 비용의 균형\n",
    "- separators 순서가 분할 품질 결정\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n프로그램 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7e4b1",
   "metadata": {},
   "source": [
    "### 3. TokenTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e322ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 파일 읽기\n",
    "with open(\"./data/ai-terminology.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일 내용을 읽어오기\n",
    "\n",
    "#print(\"원본 텍스트 미리보기:\\n\", file[:500])  # 앞 500자 출력\n",
    "\n",
    "# TokenTextSplitter 설정\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,  # 청크 크기\n",
    "    chunk_overlap=20,  # 청크 간 겹치는 부분 추가하여 문맥 유지\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI tiktoken 기본 인코딩 사용 (한글 처리 개선)\n",
    "    add_start_index=True  # 각 청크의 시작 인덱스 반환\n",
    ")\n",
    "\n",
    "# 텍스트 분할 실행\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n 총 {len(texts)}개의 청크로 분할됨.\")\n",
    "print(\"\\n 첫 번째 청크:\\n\", texts[0])\n",
    "\n",
    "# 청크 길이 확인\n",
    "for i, chunk in enumerate(texts[:5]):  # 처음 5개만 확인\n",
    "    print(f\"\\n Chunk {i+1} (길이: {len(chunk)}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        ai_terminology_text = f.read()\n",
    "    print(f\"파일 읽기 성공: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 읽기 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\")\n",
    "print(\"-\" * 50)\n",
    "print(ai_terminology_text[:500] + \"...\")\n",
    "print(f\"\\n전체 텍스트 길이: {len(ai_terminology_text)}자\")\n",
    "\n",
    "# ==========================================\n",
    "# TokenTextSplitter vs 다른 Splitter 비교\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TokenTextSplitter 특징 및 다른 Splitter와 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 토큰 개수 확인 (tiktoken 사용)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = encoding.encode(ai_terminology_text)\n",
    "print(f\"\\n원본 텍스트의 토큰 개수: {len(tokens)}개\")\n",
    "print(f\"문자 대 토큰 비율: {len(ai_terminology_text)/len(tokens):.2f} (문자/토큰)\")\n",
    "\n",
    "# 2. TokenTextSplitter 설정 및 실행\n",
    "print(\"\\n1. TokenTextSplitter (토큰 기반 분할):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "token_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,              # 토큰 개수로 크기 지정\n",
    "    chunk_overlap=20,            # 토큰 단위 중복\n",
    "    encoding_name=\"cl100k_base\", # OpenAI GPT 모델용 인코딩\n",
    "    add_start_index=True         # 시작 인덱스 정보 포함\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(token_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(token_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")\n",
    "\n",
    "# 3. 다른 Splitter와 비교\n",
    "print(\"\\n2. RecursiveCharacterTextSplitter (문자 기반 분할):\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # 문자 개수로 크기 지정 (토큰 200개 ≈ 문자 800개)\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(char_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(char_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 토큰 기반 분할의 장점 확인\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토큰 기반 분할의 장점\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LLM 토큰 제한 시뮬레이션\n",
    "max_tokens = 200  # 가상의 토큰 제한\n",
    "\n",
    "print(f\"\\nLLM 토큰 제한: {max_tokens}개 토큰\")\n",
    "print(\"\\n토큰 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n",
    "\n",
    "print(\"\\n문자 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 다양한 encoding 테스트\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"다양한 인코딩 방식 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encodings = [\n",
    "    (\"cl100k_base\", \"GPT-4, GPT-3.5-turbo\"),\n",
    "    (\"p50k_base\", \"GPT-3 (davinci)\"),\n",
    "    (\"r50k_base\", \"GPT-3 (ada, babbage, curie)\")\n",
    "]\n",
    "\n",
    "test_text = \"안녕하세요! AI와 머신러닝에 대해 배워보겠습니다. Hello, let's learn about AI and Machine Learning!\"\n",
    "\n",
    "for encoding_name, description in encodings:\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = enc.encode(test_text)\n",
    "    print(f\"\\n{encoding_name} ({description}):\")\n",
    "    print(f\"  토큰 수: {len(tokens)}개\")\n",
    "    print(f\"  토큰 예시: {tokens[:10]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 활용 예제\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"실무 활용 예제\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 다양한 chunk_size로 테스트\n",
    "chunk_sizes = [100, 200, 500]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={chunk_size} 토큰:\")\n",
    "    splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//10,  # 10% 중복\n",
    "        encoding_name=\"cl100k_base\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(ai_terminology_text)\n",
    "    avg_tokens = sum(len(encoding.encode(chunk)) for chunk in chunks) / len(chunks)\n",
    "    \n",
    "    print(f\"  총 청크 수: {len(chunks)}개\")\n",
    "    print(f\"  평균 토큰 수: {avg_tokens:.1f}개\")\n",
    "    print(f\"  첫 번째 청크 토큰 수: {len(encoding.encode(chunks[0]))}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 메타데이터 포함 문서 분할\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"메타데이터 포함 Document 분할\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document 객체 생성\n",
    "document = Document(\n",
    "    page_content=ai_terminology_text,\n",
    "    metadata={\"source\": \"ai-terminology.txt\", \"type\": \"glossary\"}\n",
    ")\n",
    "\n",
    "# Document 분할\n",
    "doc_chunks = token_splitter.split_documents([document])\n",
    "\n",
    "print(f\"Document 분할 결과: {len(doc_chunks)}개 청크\")\n",
    "for i, doc_chunk in enumerate(doc_chunks[:2], 1):\n",
    "    tokens_count = len(encoding.encode(doc_chunk.page_content))\n",
    "    print(f\"\\nDocument Chunk {i}:\")\n",
    "    print(f\"  토큰 수: {tokens_count}개\")\n",
    "    print(f\"  메타데이터: {doc_chunk.metadata}\")\n",
    "    print(f\"  내용: {doc_chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c1771",
   "metadata": {},
   "source": [
    "### 4. HuggingFace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ccc3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\Users\\user\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-kGdHTiMZ-py3.12\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 원본 텍스트 미리보기:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Faceboo\n",
      "\n",
      " 총 23개의 청크로 분할됨\n",
      "\n",
      " Chunk 1 (25자):\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      " Chunk 2 (157자):\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      " Chunk 3 (37자):\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      " Chunk 4 (176자):\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      " Chunk 5 (143자):\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "\n",
      " 첫 번째 청크의 토큰 개수: 23\n",
      " 첫 번째 청크의 토큰 리스트: ['Sem', 'antic', 'ĠSearch', 'Ġ(', 'ìĿ', 'ĺ', 'ë', '¯', '¸', 'ë', '¡', 'ł', 'ì', 'ł', 'ģ', 'Ġ', 'ê', '²', 'Ģ', 'ì']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\" 원본 텍스트 미리보기:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # 각 청크 크기 (토큰 기준 아님)\n",
    "    chunk_overlap=50,  # 청크 간 중복 부분\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "print(f\"\\n 총 {len(split_texts)}개의 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # 처음 5개만 출력\n",
    "    print(f\" Chunk {i+1} ({len(chunk)}자):\\n{chunk}\\n\")\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰 단위로 변환하여 확인\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\n 첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\" 첫 번째 청크의 토큰 리스트:\", tokenized_example[:20])  # 앞 20개만 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
